<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Every Shot Counts: Using  Exemplars for Repetition Counting in Videos</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Every Shot Counts: Using  Exemplars for Repetition Counting in Videos" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://mwray.github.io/FGAR/" />
<meta property="og:url" content="https://mwray.github.io/FGAR/" />
<meta property="og:site_name" content="Every Shot Counts: Using  Exemplars for Repetition Counting in Videos" />
<script type="application/ld+json">
{"@type":"WebSite","headline":"Every Shot Counts: Using  Exemplars for Repetition Counting in Videos","url":"https://s.github.io/FGAR/","name":"Fine-Grained Action Retrieval through Multiple Parts-of-Speech Embeddings","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=0.45, maximum-scale=1.0">
    <meta name="theme-color" content="#157878">
    <!-- <link rel="stylesheet" href="title_name_style.css"> -->
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name" >Every Shot Counts: Using  Exemplars for Repetition Counting in Videos</h1>
      <h2 class="project-tagline"></h2>
      
      
    </section>

    <section class="main-content">
      <p align="center" style="font-size:30px">
    <a href="https://sinhasaptarshi.github.io/">Saptarshi Sinha</a><sup>1</sup>, <a href="https://alexandrosstergiou.github.io/">Alex Stergiou</a><sup>2</sup> and <a href="https://dimadamen.github.io/">Dima Damen</a><sup>1</sup>
</p>
<p align="center" style="font-size:20px">
    <sup>1</sup>University of Bristol, <sup>2</sup>University of Twente
</p>

<p align="center"><img width=1000px src="images/landing_figure.png" alt="Overview" /></p>

<h2 id="abstract">Abstract</h2>
<p align="left" align="right" id="common_text" display="inline-block"> Video repetition counting infers the number of repetitions of recurring actions or motion within a video.
We propose an exemplar-based approach that discovers visual correspondence of video exemplars across repetitions
within target videos. Our proposed <b>E</b>very <b>S</b>hot <b>Counts</b> (ESCounts) model is an attention-based 
encoder-decoder that encodes videos of varying lengths alongside exemplars from the same and different videos. In 
training, ESCounts regresses locations of high correspondence to the exemplars within the video. In tandem, our method 
learns a latent that encodes representations of general repetitive motions, which we use for exemplar-free, zero-shot inference. 
Extensive experiments over commonly used datasets (RepCount, Countix, and UCFRep) showcase ESCounts obtaining state-of-the-art 
performance across all three datasets. On RepCount, ESCounts increases the off-by-one from <b>0.39</b> to <b>0.56</b> and decreases 
the mean absolute error from <b>0.38</b> to <b>0.21</b>. Detailed ablations further demonstrate the effectiveness of our method. 
</p>

<h2 id="model_overview">Model Overview</h2>
<p align="center"><img width=1000px src="images/architecture_diagram_v2.png" alt="Overview" /></p>
<!-- 
<p>In this paper, we propose to enrich the embedding by disentangling
parts-of-speech (PoS) in the accompanying captions.  We build a separate
multi-modal embedding space for each PoS tag. The outputs of multiple PoS
embeddings are then used as input to an integrated multi-modal space, where we
perform action retrieval.  All embeddings are trained jointly through a
combination of PoS-aware and PoS-agnostic losses.  Our proposal enables
learning specialised embedding spaces that offer multiple views of the same
embedded entities.</p>

<p>We report the first retrieval results on fine-grained actions for the
large-scale EPIC dataset, in a generalised zero-shot setting. Results show the
advantage of our approach for both video-to-text and text-to-video action
retrieval.</p>

<p>We also demonstrate the benefit of disentangling the PoS for the generic task
of cross-modal video retrieval on the MSR-VTT dataset.</p> -->

<h2 id="demo_videos">Demo Videos</h2>

<style>
    .embed-container {
        position: relative;
        padding-bottom: 56.25%;
        height: 0;
        overflow: hidden;
        max-width: 50%;
    }
    .embed-container iframe,
    .embed-container object,
    .embed-container embed {
        position: absolute;
        top: 0;
        left: 200px;
        right: 200px;
        width: 60%;
        height: 60%;
    }
</style>

<!-- <div class="embed-container">
    <iframe src="1_stu2_39_v2.mp4" frameborder="0" allowfullscreen></iframe>
</div>
<div class="embed-container">
    <iframe src="2_v_SoccerJuggling_g25_c03_v2.mp4" frameborder="0" allowfullscreen></iframe>
</div> -->

<div id="wrapper"> 
    <video id="home1" width="630" height="738" controls="controls" > 
        <source type="video/mp4" src="videos/1_stu2_39_v2.mp4" /> 
    </video>
    <video id="home2" width="630" height="738" controls="controls" > 
        <source type="video/mp4" src="videos/2_v_SoccerJuggling_g25_c03_v2.mp4" />
    <div class="clear"></div> 
</div>
<div id="wrapper"> 
    <video id="home1" width="630" height="738" controls="controls" > 
        <source type="video/mp4" src="videos/3_b7BkxSujNig_0.00000_10.00000_v2.mp4" /> 
    </video>
    <video id="home2" width="630" height="738" controls="controls" > 
        <source type="video/mp4" src="videos/5_stu5_47_v2.mp4" />
    <div class="clear"></div> 
</div>
<div id="wrapper"> 
    <video id="home1" width="630" height="738" controls="controls" > 
        <source type="video/mp4" src="videos/9_stu4_19_v2.mp4" /> 
    </video>
    <video id="home2" width="630" height="738" controls="controls" > 
        <source type="video/mp4" src="videos/6_v_TableTennisShot_g25_c07_v2.mp4" />
    <div class="clear"></div> 
</div>
<br>
<br>
<br>

<h2 id="common">Paper</h2>

<!-- <p><a href="/FGAR/FGAR.pdf">PDF</a></p>

<p><a href="https://arxiv.org/abs/1908.03477">ArXiv</a></p> -->

<!-- <h2 id="poster">Poster</h2>

<p>Poster presented at ICCV can be found <a href="/FGAR/FGAR_poster.pdf">here</a></p> -->

<h2 id="common">Code and Features</h2>

<p id="common_text">The code and download links for the models can be found <a href="https://anonymous.4open.science/r/EScounts-AD94">here</a></p> 
    <!-- <a href="https://github.com/mwray/Joint-Part-of-Speech-Embeddings">here</a>.</p> -->

<h2 id="common">Bibtex</h2>

<!-- <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{wray2019fine,
    author    = {Wray, Michael and Larlus, Diane and Csurka, Gabriela and Damen, Dima},
    title     = {Fine-Grained Action Retrieval through Multiple Parts-of-Speech Embeddings},
    booktitle = {IEEE/CVF International Conference on Computer Vision (ICCV)},
    year      = {2019}
}s
</code></pre></div></div> -->

<h2 id="common">Acknowledgements</h2>
<p id="common_text">Work used publicly available datasets.
Research is supported by EPSRC DTP (Doctoral Training Program) and EPSRC UMPIRE (EP/T004991/1).</p>


      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
